c("Est" = est1, "SE" = se1, "95%L" = est1 - 1.96*se1, "95%U" = est1 + 1.96*se1)
# Contrast 2: Estimate for treatment group.
contrast2 <- c(1, 1, 0, 0, prob.d[2,2], prob.d[2,3])
est2 <- contrast2 %*% pmm.mod$coefficients$fixed[1:6]
se2 <- sqrt(t(contrast2) %*% FE.vcov %*% (contrast2))
c("Est" = est2, "SE" = se2, "95%L" = est2 - 1.96*se2, "95%U" = est2 + 1.96*se2)
# Contrast 3: Placebo vs treatment group.
contrast4 <- c(0, 1, 0, 0, prob.d[2,2] - prob.d[1,2], prob.d[2,3] - prob.d[1,3])
est4 <- contrast4 %*% pmm.mod$coefficients$fixed[1:6]
se4 <- sqrt(t(contrast4) %*% FE.vcov %*% (contrast4))
c("Est" = est4, "SE" = se4, "95%L" = est4 - 1.96*se4, "95%U" = est4 + 1.96*se4)
# to run the code in this chunk, you may need to install/update the "dplyr" package
names(dat)
dat$dgrp <- NULL
library(mice)
library(mice)
md.pattern(dat)
imp0 <- mice(dat, method = "mean", m = 1, maxit = 1, print=FALSE)
md.pattern(dat)
imp0 <- mice(dat, method = "mean", m = 1, maxit = 1, print=FALSE)
colMeans(dat[,2:7], na.rm = TRUE)
imp0 <- mice(dat, method = "mean", m = 1, maxit = 1, print=FALSE)
colMeans(dat[,2:7], na.rm = TRUE)
head(dat)
head(complete(imp0))
summary(imp0)
fit0 <- with(imp0, lm(y52 ~ y0 + group))
summary(fit0)
#
imputed.dat2 <- mice(data = dat, m = 10, print=FALSE)
#
imputed.dat2 <- mice(data = dat, m = 10, print=FALSE)
imputed.mods2 <- with(imputed.dat2, expr = lm(y52 ~ y0 + group))
pooled.res2 <- pool(imputed.mods2)
summary(pooled.res2)
print(pool(imputed.mods2))
print(pool(imputed.mods2))
#
imputed.dat3 <- mice(data = dat, m = 10,method=c("norm.predict"),print=FALSE)
imputed.dat3$predictorMatrix
imputed.mods3 <- with(imputed.dat3, expr = lm(y52 ~ y0 + group))
imputed.mods3 <- with(imputed.dat3, expr = lm(y52 ~ y0 + group))
pooled.res3 <- pool(imputed.mods3)
print(pool(imputed.mods2))
#
imputed.dat3 <- mice(data = dat, m = 10,method=c("norm.predict"),print=FALSE)
imputed.dat3$predictorMatrix
imputed.mods3 <- with(imputed.dat3, expr = lm(y52 ~ y0 + group))
pooled.res3 <- pool(imputed.mods3)
summary(pooled.res3)
print(pool(imputed.mods3))
summary(fit0)
fit0
summary(fit0)
library(mice)
?mice
print(pool(imputed.mods2))
summary(pooled.res2)
summary(pooled.res3)
imp0
head(complete(imp0))
complete(imp0)
head(complete(imp0))
imputed.dat2
imputed.mods2
imputed.dat2
complete(imputed.dat2)
head(complete(imp0))
# We ignore Week 1 since it only has n=4 missing values
#
# Generate indicator variables for missingness
r1 <- 1 - is.na(dat$y4) # =0 if missing
r2 <- 1 - is.na(dat$y12)
r4 <- 1 - is.na(dat$y24)
r6 <- 1 - is.na(dat$y52)
# These are the Bernoulli random variables that we model, in order to obtain the weights
#
############
## WEEK 4 ##
############
# Generate level and trend variables
level2 <- dat$y4 + dat$y0
trend2 <- dat$y4 - dat$y0
#
# Fit a logistic regression to estimate probabilities of missingness at week 2 based on covariates
# We model using the data in which the week 1 measurement was observed
logit.mod2a <- glm(r2~group+level2+trend2+y0, data = dat, subset = (r1==1), family = "binomial")
#
# The `predict` function gives the estimated probability of non-misssing given predictors
dat$w2b[r1==1] <- 1/predict(logit.mod2a, type = "response")[r1==1]
# Now for the stabilized weights
logit.mod2b <- glm(r2~group+y0, data = dat, subset = (r1==1), family = "binomial")
dat$w2[r1==1] <- predict(logit.mod2b,type = "response")/predict(logit.mod2a, type = "response")
#
############
## WEEK 12 ##
############
# Generate level and trend variables
level4 <- dat$y12 + dat$y4
trend4 <- dat$y12 - dat$y4
# Fit a logistic regression
logit.mod4a <- glm(r4~group+level4+trend4+y0, data = dat, subset = (r2==1), family = "binomial")
dat$w4b[r2==1] <- 1/predict(logit.mod4a, type = "response")[r2==1]
logit.mod4b <- glm(r4~group+y0, data = dat, subset = (r2==1), family = "binomial")
dat$w4[r2==1] <- predict(logit.mod4b, type = "response")/predict(logit.mod4a, type = "response")[r2==1]
############
## WEEK 24 ##
############
# Generate level and trend variables
level6 <- dat$y24 + dat$y12
trend6 <- dat$y24 - dat$y12
# Fit a logistic regression
logit.mod6a <- glm(r6~group+level6+trend6+y0, data = dat, subset = (r4==1), family = "binomial")
dat$w6b[r4==1 & !is.na(level6)] <- 1/predict(logit.mod6a, type = "response")
logit.mod6b <- glm(r6~group+y0, data = dat, subset = (r4==1), family = "binomial")
dat$w6[r4==1 & !is.na(level6)] <- predict(logit.mod6b, type = "response", newdata = dat)[!is.na(level6)]/predict(logit.mod6a, type = "response")
# Stabalized weights
dat$wgt <- dat$w2 * dat$w4 * dat$w6
lin.mod.wgt <- lm(y52 ~ group + y0, data = dat, weights = wgt)
summary(lin.mod.wgt)
# We can alternatively obtain the robust SE
#install.packages("sandwich")
library(sandwich)
robust_se.wgt <- sqrt(diag(vcovHC(lin.mod.wgt, type = "HC0")))
signif(robust_se.wgt,3)
summary(lin.mod.wgt)
# Non-stabalized weights
dat$wgt2 <- dat$w2b * dat$w4b * dat$w6b
lin.mod.wgt2 <- lm(y52 ~ group + y0, data = dat, weights = wgt2)
summary(lin.mod.wgt2)
# Again use vcov to obtain robust SE
robust_se.wgt2 <- sqrt(diag(vcovHC(lin.mod.wgt2, type = "HC0")))
signif(robust_se.wgt2,3)
library(lme4)
library(gee)
install.packages("gee")
library(gee)
set.seed(4)
expit <- function(x) {1/(1+exp(-x)) }
beta1 <- 1
n.class <- 20
n.groups <- 20*n.class
n.subjs <- 20*n.groups
n.total <- 3*n.subjs
class <- rep(1:n.class, each=n.total/n.class)
group <- rep(1:n.groups, each=n.total/n.groups)
subj <- rep(1:n.subjs, each=n.total/n.subjs)
gamma.class <- rep(rnorm(n.class), each=n.total/n.class)
gamma.group <- rep(rnorm(n.groups), each=n.total/n.groups)
gamma.subj <- rep(rnorm(n.subjs), each=n.total/n.subjs)
x <- rnorm(n.total)
n.total/n.class
gamma.group <- rep(rnorm(n.groups), each=n.total/n.groups)
gamma.subj <- rep(rnorm(n.subjs), each=n.total/n.subjs)
x <- rnorm(n.total)
p <- expit(beta1*x+gamma.class+gamma.group+gamma.subj)
y <- rbinom(n.total,1,p)
m1.gee <- gee(y~x, id=class, family="binomial", corstr="independence")
m1a.gee <- gee(y~x, id=class, family="binomial", corstr="exchangeable")
summary(m1.gee)$coef
summary(m1a.gee)$coef
m1.glmm <- glmer(y~x + (1 | class) , family=binomial)
m2.glmm <- glmer(y~x + (1 | class) + (1|group), family=binomial)
m3.glmm <- glmer(y~x + (1 | class) + (1|group) + (1|subj), family=binomial)
X = rnorm(100)
acf(X)
log(0.01)/log(0.8)
?pt
pt(-2*sqrt(5),10)
##### Check structure of time series
y = AirPassengers
##### Load Packages
require(forecast) #seasonplot
require(fda) #create bases for functional data
require(rainbow) #for coloring curves
require(qcc) #ewma control chart
require(car) #better QQ plots
##### Check structure of time series
y = AirPassengers
#frequency should be 12 for monthly
frequency(12)
#frequency should be 12 for monthly
frequency(12)
#look at range of values
summary(y)
y
?frequency
#look at start and end portion of series
start(y)
end(y)
#check if any gaps in time series
v_time = seq( as.Date("1949-01-01"), as.Date("1960-12-01"), by='1 month'  )
length(y)==length(v_time)
length(y)
##### Create dataframe with basic time features
d = data.frame(Time = v_time, y)
d$Year = as.integer(strftime(v_time,'%Y'))
d$Month = as.integer(strftime(v_time,'%m'))
##### Basic Univariate Plots
plot(y)
seasonplot(y) #look for consistency in peak/offpeak
#look at annual data to better see smooth trend
d_yr = aggregate(data = d, y ~ Year, sum  )
#look at annual data to better see smooth trend
d_yr = aggregate(data = d, y ~ Year, sum  )
= aggregate(data = d, y ~ Year, sum  )
d_yr
ts.plot(d_yr$y)
v_yr = sort(unique(d$Year))
#check if monthly proportions roughly look constant year-to-year
colnames(d_yr)[2] = "y_annual"
d = merge(d, d_yr, by = "Year")
d_yr
d$y_prop
d
d$y_prop = d$y/d$y_annual
ts.plot(d$y_prop)
#Look at distribution in monthly proportions (assuming stationary)
boxplot(data = d, y_prop ~ Month) ##check similarity in proportions
##### STL Decompostion
#given seasonal effect depends on level do log transform
y_log = log(y)
#decompose and plot
fit_stl = stl(y_log, s.window = 12)
plot(fit_stl)
#extract components
S = fit_stl$time.series[,1]
L = fit_stl$time.series[,2]
E = fit_stl$time.series[,3]
ts.plot(exp(S), main = 'Multiplicative Seasonality Component')
##### Functional Data View of Seasonality
#convert univariate series to 12 curves for each year
M = matrix(NA, nrow = 12, ncol =  length(v_yr) )
d$S = exp(S)
##### Functional Data View of Seasonality
#convert univariate series to 12 curves for each year
M = matrix(NA, nrow = 12, ncol =  length(v_yr) )
v_yr
d$S = exp(S)
for( j in 1:ncol(M)  ){
#subset
d_j = subset(d, Year == v_yr[j])
S_j = d_j$S
S_j = S_j/mean(S_j) #ensure normalized
#add
M[,j] = S_j
}
colnames(M) = v_yr
#Add Fourier Bases and plot
v_f <- create.fourier.basis(  rangeval = c(0,nrow(M)),  nbasis = 7)
S_basis <- smooth.basis( y = M, fdParobj = v_f)
plot(S_basis$fd)
data(Collage)
data(College)
M
subset(d, Year == v_yr[j])
plot(fit_stl)
S_j = d_j$S
mean(S_j)
#Add Fourier Bases and plot
v_f <- create.fourier.basis(  rangeval = c(0,nrow(M)),  nbasis = 7)
S_basis <- smooth.basis( y = M, fdParobj = v_f)
plot(S_basis$fd)
#Add mean
W.mean <- mean.fd(S_basis$fd)
lines(W.mean, lty = 2, col='black', lwd=3)
##### Look at Multiplicative Residuals
#plot centered about zero to eyeball randomness
ts.plot(E)
tSeq = 1:length(E)
abline(h=0, col='black')
#autocorrelation
acf(E)
#control chart at slow smoothing
ewma(E, lambda = .1)
#fit ar(1) model and extract residuals
fit_ar1 = arima(E, order = c(1,0,0))
r = fit_ar1$residuals
r = r/sd(r)
#distribution
hist(r, main='Frequency of Normalized Multiplicative Residuals')
#compare fit against standard Gaussian
qqPlot(r, main='Normalized Multiplicative Residuals vs N(0,1)')
?plas
library(Plasmode)
?plas
n <- 100
x <- rbinom(n,p=0.25)
x <- rbinom(size=n,p=0.25)
x <- rbinom(n,size=1,p=0.25)
hist(x)
x
x <- rbinom(n,size=30,p=0.25)
hist(x)
hist(x/30)
x <- rbinom(n,size=300,p=0.25)
hist(x/300)
n <- 1000
x <- rbinom(n,size=300,p=0.25)
hist(x/300)
hist(x/300,xlim=c(0,1))
n <- 100
x <- rbinom(n,size=300,p=0.25)
hist(x/300,xlim=c(0,1))
n <- 10
x <- rbinom(n,size=300,p=0.25)
hist(x/300,xlim=c(0,1))
n <- 100
x <- rbinom(n,size=300,p=0.25)
hist(x/300,xlim=c(0,1))
hist(x/300)
pnorm(0.975)
p\qnorm(0.975)
qnorm(0.975)
getwd()
x<-c(46,47,45,45,47)
var(x)
?t.test
library(tidyverse)
library(ggplot2)
library(haven)
library(readxl)
library(table1)
library(ggbeeswarm)
library(ltmle)
library(Plasmode)
library(SuperLearner)
library(ctmle)
library(sl3)
library(tmle3)
library(tmle3)
library(tictoc)
library(doParallel)
library(doRNG)
library(foreach)
library(randomForest)
path <- "~/Desktop/HuangGroup/cvtmle_plasmode"
setwd(paste0(path,"/Code"))
set.seed(42782)
options(tibble.print_max = 40, tibble.print_min = 30)
no_cores <- detectCores(all.tests = T) - 2
registerDoParallel(cores=no_cores)
# Set simulation parameters
{
sims.ver <- "plas"
# sims.ver <- "5var"
# sims.ver <- "5var.then.plas"
Effect_Size <- 6.6
plas.seed <- 1111
########
# parameters for plasmode
#######
# p=331 for the whole set
data.ver <- "FULL"
# data.ver <- "13"
size <- 1178
# size <-200
plas_sim_N <- 500; use.subset <- F
generateA <- T
estimateWithMore <- F; p.sim = 100;p.est <- 50
randVar = T # Do we permute variable order?
########
# parameters for 5 var, 5var.then.plas
#######
# Nsets <- 10000
# Nsamp <- 3000
Nsets <- 500
Nsamp <- 600
}
source("20200803-Sims-Function.R")
cat(sims.ver)
sims.obj <- general.sim(sims.ver)
if (sims.ver == "plas"|sims.ver == "5var.then.plas"){
plas <- sims.obj$plas
plas_sims <- sims.obj$plas_sims
vars <- sims.obj$vars
}else{
sim_boots <- sims.obj
}
plot(plas_sims$TrueOutBeta)
##################################
## PARALLELIZE ANALYSES
##################################
# Set param
{
# DC implementation
source("20200705-DCDR-Functions.R")
# getRES function is now relocated to a separate file (20200720-Algos-code.R)
source("20200720-Algos-code.R")
non.par <- F
# specify which set of learners for SL
if (non.par == T){
### NON-SMOOTH
short_tmle_lib <- SL_param_list
tmle_lib <- lrnr_SL
aipw_lib <- SL.lib
}
else{
# SMOOTH
short_tmle_lib <- SL_list
tmle_lib <- lrnr_SL_param
aipw_lib <- SL.param
}
aipw_lib <- c("SL.glmnet")
errorhandling="stop"
# errorhandling="remove"
N_sims <- 2# this should <= plas_sim_N
doIPW = 0; doLASSO=0;
doAIPW=0; doDCAIPW=0
doManuTMLE=1; doShortTMLE = 0;
doDCTMLE=0
num_cf=5
#control=list()
control=SuperLearner.CV.control(V=2)
}
##################################
## PARALLELIZE ANALYSES
##################################
# Set param
{
# DC implementation
source("20200705-DCDR-Functions.R")
# getRES function is now relocated to a separate file (20200720-Algos-code.R)
source("20200720-Algos-code.R")
non.par <- F
# specify which set of learners for SL
if (non.par == T){
### NON-SMOOTH
short_tmle_lib <- SL_param_list
tmle_lib <- lrnr_SL
aipw_lib <- SL.lib
}
else{
# SMOOTH
short_tmle_lib <- SL_list
tmle_lib <- lrnr_SL_param
aipw_lib <- SL.param
}
aipw_lib <- c("SL.glmnet")
errorhandling="stop"
# errorhandling="remove"
N_sims <- 50# this should <= plas_sim_N
doIPW = 0; doLASSO=0;
doAIPW=0; doDCAIPW=0
doManuTMLE=1; doShortTMLE = 0;
doDCTMLE=1
num_cf=5
#control=list()
control=SuperLearner.CV.control(V=2)
}
#########
# run
##########
source("20200904-run-sim-code.R")
# print(paste("It takes",round((ptm1 - ptm0)[3],2),"s"))
print(paste("Cores used:",no_cores))
print(paste("plas.seed=",plas.seed, "generateA=",generateA, "sims.ver=",sims.ver))
save(boot1, file=paste("./RDataFiles/091220.RData",sep=""))
getwd()
path
{
source("20200816-Result-Summary.R")
# path <- "~/Desktop/HuangGroup/cvtmle_plasmode/Code/cluster/submitted091320/"
load(paste0(path,"RDataFiles/091320.RData"))
summarise.res(boot1)
}
# path <- "~/Desktop/HuangGroup/cvtmle_plasmode/Code/cluster/submitted091320/"
path <- "/Users/garethalex/Desktop/HuangGroup/cvtmle_plasmode/Code/"
load(paste0(path,"RDataFiles/091320.RData"))
summarise.res(boot1)
load(paste0(path,"RDataFiles/091220.RData"))
summarise.res(boot1)
##################################
## PARALLELIZE ANALYSES
##################################
# Set param
{
# DC implementation
source("20200705-DCDR-Functions.R")
# getRES function is now relocated to a separate file (20200720-Algos-code.R)
source("20200720-Algos-code.R")
non.par <- F
# specify which set of learners for SL
if (non.par == T){
### NON-SMOOTH
short_tmle_lib <- SL_param_list
tmle_lib <- lrnr_SL
aipw_lib <- SL.lib
}
else{
# SMOOTH
short_tmle_lib <- SL_list
tmle_lib <- lrnr_SL_param
aipw_lib <- SL.param
}
aipw_lib <- c("SL.glmnet")
errorhandling="stop"
# errorhandling="remove"
N_sims <- 50# this should <= plas_sim_N
doIPW = 0; doLASSO=1;
doAIPW=0; doDCAIPW=0
doManuTMLE=0; doShortTMLE = 0;
doDCTMLE=0
num_cf=5
#control=list()
control=SuperLearner.CV.control(V=2)
}
#########
# run
##########
source("20200904-run-sim-code.R")
summarise.res(boot1)
